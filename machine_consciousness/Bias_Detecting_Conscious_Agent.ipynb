{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8Smy7wM1rkGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # **Bias-Detecting Conscious Agent**\n",
        "This is a part of the large product for machine conscious product. Python demo code for a Bias-Detecting Conscious Agent. It:\n",
        "\n",
        "1.\tSimulates hiring decisions with potential gender bias.\n",
        "\n",
        "2.\tChecks if a profile is treated unfairly based on gender.\n",
        "\n",
        "3.\tRecommends a fairness action if bias is found.\n"
      ],
      "metadata": {
        "id": "Wwlh8jerr2fK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWH7stySqCx8",
        "outputId": "120a5d8a-1c6f-42b8-f9f5-cbc330e76b29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original Data:\n",
            "   Education  Experience  Gender Hired\n",
            "0    Masters           5    Male   Yes\n",
            "1  Bachelors           2  Female    No\n",
            "2        PhD           7    Male   Yes\n",
            "3  Bachelors           3  Female    No\n",
            "4    Masters           4    Male   Yes\n",
            "\n",
            "Bias Test - Same Profile, Different Gender:\n",
            "Male Applicant: Yes\n",
            "Female Applicant: No\n",
            "\n",
            "⚠️ Bias Detected in Hiring Decision based on Gender!\n",
            "✅ Recommendation: Review hiring criteria or anonymize gender during evaluation.\n",
            "\n",
            "Model Explanation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         2\n",
            "           1       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           1.00         5\n",
            "   macro avg       1.00      1.00      1.00         5\n",
            "weighted avg       1.00      1.00      1.00         5\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Simple Python Demo: Bias-Detecting Conscious Agent (Google Colab Ready)\n",
        "\n",
        "# Objective:\n",
        "# Detect bias in hiring decisions based on gender and recommend fair alternatives.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Step 1: Sample Hiring Data (with gender bias)\n",
        "data = pd.DataFrame({\n",
        "    'Education': ['Masters', 'Bachelors', 'PhD', 'Bachelors', 'Masters'],\n",
        "    'Experience': [5, 2, 7, 3, 4],\n",
        "    'Gender': ['Male', 'Female', 'Male', 'Female', 'Male'],\n",
        "    'Hired': ['Yes', 'No', 'Yes', 'No', 'Yes']\n",
        "})\n",
        "\n",
        "print(\"\\nOriginal Data:\")\n",
        "print(data)\n",
        "\n",
        "# Step 2: Encode categorical variables using separate encoders\n",
        "le_education = LabelEncoder()\n",
        "le_gender = LabelEncoder()\n",
        "le_hired = LabelEncoder()\n",
        "\n",
        "data['Education'] = le_education.fit_transform(data['Education'])\n",
        "data['Gender'] = le_gender.fit_transform(data['Gender'])\n",
        "data['Hired'] = le_hired.fit_transform(data['Hired'])  # Yes = 1, No = 0\n",
        "\n",
        "\n",
        "# Step 3: Split Features and Target\n",
        "X = data[['Education', 'Experience', 'Gender']]\n",
        "y = data['Hired']\n",
        "\n",
        "# Step 4: Train a Simple Classifier\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Step 5: Simulate Bias Detection (Check gender impact)\n",
        "def simulate_applicant(ed, exp, gender):\n",
        "    try:\n",
        "        ed_encoded = le_education.transform([ed])[0]\n",
        "        gender_encoded = le_gender.transform([gender])[0]\n",
        "    except ValueError as e:\n",
        "        return f\"❌ Error: {e}\"\n",
        "\n",
        "    prediction = model.predict([[ed_encoded, exp, gender_encoded]])[0]\n",
        "    label = le_hired.inverse_transform([prediction])[0]\n",
        "    return label\n",
        "\n",
        "\n",
        "# Evaluate same profile for Male vs Female\n",
        "print(\"\\nBias Test - Same Profile, Different Gender:\")\n",
        "print(\"Male Applicant:\", simulate_applicant('Masters', 4, 'Male'))\n",
        "print(\"Female Applicant:\", simulate_applicant('Masters', 4, 'Female'))\n",
        "\n",
        "# Step 6: Recommendation if Bias Detected\n",
        "def check_bias():\n",
        "    male_result = simulate_applicant('Masters', 4, 'Male')\n",
        "    female_result = simulate_applicant('Masters', 4, 'Female')\n",
        "    if male_result != female_result:\n",
        "        print(\"\\n⚠️ Bias Detected in Hiring Decision based on Gender!\")\n",
        "        print(\"✅ Recommendation: Review hiring criteria or anonymize gender during evaluation.\")\n",
        "    else:\n",
        "        print(\"\\n✅ No bias detected for test case.\")\n",
        "\n",
        "check_bias()\n",
        "\n",
        "# Step 7: (Optional) Explanation Report\n",
        "print(\"\\nModel Explanation:\")\n",
        "print(classification_report(y, model.predict(X)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Concept and code execution by Bhadale IT, code generated by ChatGPT **"
      ],
      "metadata": {
        "id": "p5i40u7ttDl1"
      }
    }
  ]
}